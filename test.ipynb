{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "video_dir_path = 'mario_videos_test'\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', apply_api_compatibility=True, render_mode=\"human\")\n",
    "\"\"\"\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=video_dir_path,\n",
    "    episode_trigger=lambda episode_id: True,\n",
    "    name_prefix='mario-video-{}'.format(time.ctime())\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# run 1 episode\n",
    "env.reset()\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _, info = env.step(action)\n",
    "    #time.sleep(1/30)\n",
    "    if done or info['time'] < 370:\n",
    "        break\n",
    "print(\"Your mario video is saved in {}\".format(video_dir_path))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_super_mario_bros\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "print(env.observation_space.shape)  # Dimensions of a frame\n",
    "print(env.action_space.n)  # Number of actions our agent can take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "from torchvision import transforms as T\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "    \n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "        self.transform = T.Grayscale()\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        observation = self.transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "        self.transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.transforms(observation).squeeze(0)\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from gym.wrappers import RecordVideo, FrameStack\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "\n",
    "import time\n",
    "\n",
    "video_dir_path = 'mario_videos'\n",
    "\n",
    "def make_env(env, video_dir_path=None):\n",
    "    \"\"\"Apply a series of wrappers to the environment.\"\"\"\n",
    "    env = JoypadSpace(env, RIGHT_ONLY)  # Reduce action space\n",
    "    \n",
    "    if video_dir_path is not None:\n",
    "        env = RecordVideo(\n",
    "            env,\n",
    "            video_folder=video_dir_path,\n",
    "            episode_trigger=lambda episode_id: True,\n",
    "            name_prefix='mario-video-{}'.format(time.ctime())\n",
    "        )\n",
    "    \n",
    "    #env = RecordEpisodeStatistics(env)  # Track stats\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = GrayScaleObservation(env)  # Convert to grayscale\n",
    "    env = ResizeObservation(env, 84)  # Resize to 84x84\n",
    "    env = FrameStack(env, num_stack=4)  # Stack 4 frames\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSolver(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQNSolver, self).__init__()\n",
    "        input_shape = (4, 84, 84)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(*shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.squeeze(-1)\n",
    "        conv_out = self.conv(x).reshape(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_space, action_space, max_memory_size, batch_size, gamma, lr, exploration_max, exploration_min, exploration_decay):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.max_memory_size = max_memory_size\n",
    "        self.memory_sample_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.exploration_max = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.exploration_rate = self.exploration_max\n",
    "        self.step = 0\n",
    "        self.copy = 1000  # Copy target model weights every 1000 steps\n",
    "        \n",
    "        # Memory Buffers\n",
    "        self.STATE_MEM = torch.zeros((max_memory_size, *state_space))\n",
    "        self.ACTION_MEM = torch.zeros((max_memory_size, 1))\n",
    "        self.REWARD_MEM = torch.zeros((max_memory_size, 1))\n",
    "        self.STATE2_MEM = torch.zeros((max_memory_size, *state_space))\n",
    "        self.DONE_MEM = torch.zeros((max_memory_size, 1))\n",
    "        self.ending_position = 0\n",
    "        self.num_in_queue = 0\n",
    "\n",
    "        # Neural Networks\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.local_net = DQNSolver(state_space, action_space).to(self.device)\n",
    "        self.target_net = DQNSolver(state_space, action_space).to(self.device)\n",
    "        self.target_net.load_state_dict(self.local_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.local_net.parameters(), lr=lr)\n",
    "        self.l1 = nn.SmoothL1Loss()\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.local_net.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.local_net.load_state_dict(torch.load(path))\n",
    "\n",
    "    def act(self, state, evaluate=False):\n",
    "        \"\"\"Select an action using an epsilon-greedy policy\"\"\"\n",
    "        \n",
    "        if random.random() < self.exploration_rate and not evaluate:\n",
    "            return torch.tensor([[random.randrange(self.action_space)]], dtype=torch.float32)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "            \n",
    "                state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "                return self.local_net(state.to(self.device)).argmax(dim=1, keepdim=True).cpu().float()\n",
    "\n",
    "    def copy_model(self):\n",
    "        \"\"\"Copy local network weights to target network\"\"\"\n",
    "        self.target_net.load_state_dict(self.local_net.state_dict())\n",
    "\n",
    "    def update_exploration_rate(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, state2, done):\n",
    "        self.STATE_MEM[self.ending_position] = state.float()\n",
    "        self.ACTION_MEM[self.ending_position] = action.float()\n",
    "        self.REWARD_MEM[self.ending_position] = reward.float()\n",
    "        self.STATE2_MEM[self.ending_position] = state2.float()\n",
    "        self.DONE_MEM[self.ending_position] = done.float()\n",
    "        self.ending_position = (self.ending_position + 1) % self.max_memory_size  # FIFO tensor\n",
    "        self.num_in_queue = min(self.num_in_queue + 1, self.max_memory_size)\n",
    "        \n",
    "    def recall(self):\n",
    "        # Randomly sample 'batch size' experiences\n",
    "        idx = random.choices(range(self.num_in_queue), k=self.memory_sample_size)\n",
    "        \n",
    "        STATE = self.STATE_MEM[idx].to(self.device)\n",
    "        ACTION = self.ACTION_MEM[idx].to(self.device)\n",
    "        REWARD = self.REWARD_MEM[idx].to(self.device)\n",
    "        STATE2 = self.STATE2_MEM[idx].to(self.device)\n",
    "        DONE = self.DONE_MEM[idx].to(self.device)\n",
    "        \n",
    "        return STATE, ACTION, REWARD, STATE2, DONE\n",
    "        \n",
    "    def experience_replay(self):\n",
    "        \n",
    "        if self.step % self.copy == 0:\n",
    "            self.copy_model()\n",
    "\n",
    "        if self.memory_sample_size > self.num_in_queue:\n",
    "            return\n",
    "\n",
    "        STATE, ACTION, REWARD, STATE2, DONE = self.recall()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        # Double Q-Learning target is Q*(S, A) <- r + γ max_a Q_target(S', a)\n",
    "        target = REWARD + torch.mul((self.gamma * self.target_net(STATE2).max(1).values.unsqueeze(1)), 1 - DONE)\n",
    "\n",
    "        current = self.local_net(STATE).gather(1, ACTION.long())\n",
    "        loss = self.l1(current, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent: DQNAgent, env: gym.Env, num_episodes: int = 10) -> DQNAgent:    \n",
    "    total_rewards = []\n",
    "    \n",
    "    for ep_num in tqdm(range(num_episodes)):\n",
    "        state, info = env.reset()\n",
    "\n",
    "        # State is a LazyFrame\n",
    "        state = torch.Tensor(state[0].__array__() if isinstance(state, tuple) else state.__array__())\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            state_next, reward, terminal, trunc, info = env.step(int(action[0]))\n",
    "            total_reward += reward\n",
    "            \n",
    "            state_next = torch.Tensor(state_next[0].__array__() if isinstance(state_next, tuple) else state_next.__array__())\n",
    "            reward = torch.tensor([reward])#.unsqueeze(0)\n",
    "            \n",
    "            terminal = torch.tensor([int(terminal)])#.unsqueeze(0)\n",
    "            agent.remember(state, action, reward, state_next, terminal)\n",
    "            agent.experience_replay()\n",
    "            \n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        agent.save('mario_model.pth')\n",
    "        print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBrosRandomStages-v0', apply_api_compatibility=True, render_mode='rgb_array')\n",
    "env = make_env(env)#, 'training_videos')\n",
    "observation_space = env.observation_space.shape\n",
    "action_space = env.action_space.n\n",
    "agent = DQNAgent(state_space=observation_space,\n",
    "                    action_space=action_space,\n",
    "                    max_memory_size=30000,\n",
    "                    batch_size=64,\n",
    "                    gamma=0.90,\n",
    "                    lr=0.00025,\n",
    "                    exploration_max=0.90,\n",
    "                    exploration_min=0.02,\n",
    "                    exploration_decay=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists('mario_model.pth'):\n",
    "    agent.load('mario_model.pth')\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBrosRandomStages-v0', apply_api_compatibility=True, render_mode='rgb_array')\n",
    "env = make_env(env)#, 'training_videos')\n",
    "\n",
    "trained_agent = train(agent, env, num_episodes=200)\n",
    "env.close()\n",
    "trained_agent.save('mario_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env: gym.Env, num_episodes=1, show: bool = False) -> float:\n",
    "    \"\"\"Runs the trained agent in the environment without training.\"\"\"\n",
    "    \n",
    "    total_rewards = []\n",
    "    \n",
    "    for ep_num in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        state = torch.Tensor(state[0].__array__() if isinstance(state, tuple) else state.__array__())\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state, evaluate=True)  # Use deterministic policy\n",
    "            state_next, reward, done, _, info = env.step(int(action.item()))\n",
    "            if show:\n",
    "                time.sleep(1/30)\n",
    "            done = done or info['time'] < 250\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = torch.Tensor(state_next[0].__array__() if isinstance(state_next, tuple) else state_next.__array__())\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        if show:\n",
    "            print(f\"Evaluation Episode {ep_num + 1}: Total Reward = {total_reward}\")\n",
    "    if show:\n",
    "        print(f\"Average Reward over {num_episodes} episodes: {np.mean(total_rewards)}\")\n",
    "    return np.mean(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "worlds = list(range(1, 9))\n",
    "stages = list(range(1, 5))\n",
    "rewards = []\n",
    "for world in worlds:\n",
    "    for stage in stages:\n",
    "        env = gym_super_mario_bros.make(f'SuperMarioBros-{world}-{stage}-v0', apply_api_compatibility=True, render_mode='rgb_array')\n",
    "        env = make_env(env)\n",
    "        rewards.append(evaluate_agent(trained_agent, env))\n",
    "        env.close()\n",
    "        print(\"World {} Stage {}: Reward = {}\".format(world, stage, rewards[-1]))\n",
    "\n",
    "print(f\"Average Reward over all stages: {np.mean(rewards)}\")\n",
    "\n",
    "env = gym_super_mario_bros.make(f'SuperMarioBros-1-1-v0', apply_api_compatibility=True, render_mode='human')\n",
    "env = make_env(env)\n",
    "rewards.append(evaluate_agent(trained_agent, env, show=True))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzolouv/miniconda3/envs/project_aa/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enzolouv/miniconda3/envs/project_aa/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enzolouv/miniconda3/envs/project_aa/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '__array_interface__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrecord\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m record_mario_gameplay\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This will play and record Mario with random actions\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m gif_path \u001b[38;5;241m=\u001b[39m \u001b[43mrecord_mario_gameplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGIF saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgif_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/ENS PS 2024-2025/cours/autonomous_agent/amlaa-project/record.py:35\u001b[0m, in \u001b[0;36mrecord_mario_gameplay\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m step_count \u001b[38;5;241m<\u001b[39m max_steps:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Capture the current screen\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     frame \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m---> 35\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Take a random action\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[0;32m~/miniconda3/envs/project_aa/lib/python3.8/site-packages/PIL/Image.py:3266\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfromarray\u001b[39m(obj: SupportsArrayInterface, mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image:\n\u001b[1;32m   3220\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;124;03m    Creates an image memory from an object exporting the array interface\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;124;03m    (using the buffer protocol)::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3264\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.1.6\u001b[39;00m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3266\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__array_interface__\u001b[49m\n\u001b[1;32m   3267\u001b[0m     shape \u001b[38;5;241m=\u001b[39m arr[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3268\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(shape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '__array_interface__'"
     ]
    }
   ],
   "source": [
    "from record import record_mario_gameplay\n",
    "worlds = list(range(1, 9))\n",
    "stages = list(range(1, 5))\n",
    "rewards = []\n",
    "for world in worlds:\n",
    "    for stage in stages:\n",
    "        env = gym_super_mario_bros.make(f'SuperMarioBros-{world}-{stage}-v0', apply_api_compatibility=True, render_mode='rgb_array')\n",
    "        env = make_env(env)\n",
    "        rewards.append(evaluate_agent(trained_agent, env))\n",
    "        env.close()\n",
    "        print(\"World {} Stage {}: Reward = {}\".format(world, stage, rewards[-1]))\n",
    "\n",
    "# This will play and record Mario with random actions\\n\"\n",
    "gif_path = record_mario_gameplay()\n",
    "print(f\"GIF saved to: {gif_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
