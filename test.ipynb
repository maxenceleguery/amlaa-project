{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3)\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "import gym_super_mario_bros\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v3')\n",
    "print(env.observation_space.shape)  # Dimensions of a frame\n",
    "print(env.action_space.n)  # Number of actions our agent can take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "from torchvision import transforms as T\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "    \n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "        self.transform = T.Grayscale()\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        observation = self.transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "        self.transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.transforms(observation).squeeze(0)\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from gym.wrappers import RecordVideo, FrameStack\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "\n",
    "import time\n",
    "\n",
    "video_dir_path = 'mario_videos'\n",
    "\n",
    "def make_env(env, video_dir_path=None):\n",
    "    \"\"\"Apply a series of wrappers to the environment.\"\"\"\n",
    "    env = JoypadSpace(env, RIGHT_ONLY)  # Reduce action space\n",
    "    \n",
    "    if video_dir_path is not None:\n",
    "        env = RecordVideo(\n",
    "            env,\n",
    "            video_folder=video_dir_path,\n",
    "            episode_trigger=lambda episode_id: True,\n",
    "            name_prefix='mario-video-{}'.format(time.ctime())\n",
    "        )\n",
    "    \n",
    "    #env = RecordEpisodeStatistics(env)  # Track stats\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = GrayScaleObservation(env)  # Convert to grayscale\n",
    "    env = ResizeObservation(env, 84)  # Resize to 84x84\n",
    "    env = FrameStack(env, num_stack=4)  # Stack 4 frames\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSolver(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQNSolver, self).__init__()\n",
    "        input_shape = (4, 84, 84)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(*shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.squeeze(-1)\n",
    "        conv_out = self.conv(x).reshape(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_space, action_space, max_memory_size, batch_size, gamma, lr, exploration_max, exploration_min, exploration_decay):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.max_memory_size = max_memory_size\n",
    "        self.memory_sample_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.exploration_max = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.exploration_rate = self.exploration_max\n",
    "        self.step = 0\n",
    "        self.copy = 1000  # Copy target model weights every 1000 steps\n",
    "        \n",
    "        # Memory Buffers\n",
    "        self.STATE_MEM = torch.zeros((max_memory_size, *state_space))\n",
    "        self.ACTION_MEM = torch.zeros((max_memory_size, 1))\n",
    "        self.REWARD_MEM = torch.zeros((max_memory_size, 1))\n",
    "        self.STATE2_MEM = torch.zeros((max_memory_size, *state_space))\n",
    "        self.DONE_MEM = torch.zeros((max_memory_size, 1))\n",
    "        self.ending_position = 0\n",
    "        self.num_in_queue = 0\n",
    "\n",
    "        # Neural Networks\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.local_net = DQNSolver(state_space, action_space).to(self.device)\n",
    "        self.target_net = DQNSolver(state_space, action_space).to(self.device)\n",
    "        self.target_net.load_state_dict(self.local_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.local_net.parameters(), lr=lr)\n",
    "        self.l1 = nn.SmoothL1Loss()\n",
    "\n",
    "    def act(self, state, evaluate=False):\n",
    "        \"\"\"Select an action using an epsilon-greedy policy\"\"\"\n",
    "        \n",
    "        if random.random() < self.exploration_rate and not evaluate:\n",
    "            return torch.tensor([[random.randrange(self.action_space)]], dtype=torch.float32)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "            \n",
    "                state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "                return self.local_net(state.to(self.device)).argmax(dim=1, keepdim=True).cpu().float()\n",
    "\n",
    "    def copy_model(self):\n",
    "        \"\"\"Copy local network weights to target network\"\"\"\n",
    "        self.target_net.load_state_dict(self.local_net.state_dict())\n",
    "\n",
    "    def update_exploration_rate(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, state2, done):\n",
    "        self.STATE_MEM[self.ending_position] = state.float()\n",
    "        self.ACTION_MEM[self.ending_position] = action.float()\n",
    "        self.REWARD_MEM[self.ending_position] = reward.float()\n",
    "        self.STATE2_MEM[self.ending_position] = state2.float()\n",
    "        self.DONE_MEM[self.ending_position] = done.float()\n",
    "        self.ending_position = (self.ending_position + 1) % self.max_memory_size  # FIFO tensor\n",
    "        self.num_in_queue = min(self.num_in_queue + 1, self.max_memory_size)\n",
    "        \n",
    "    def recall(self):\n",
    "        # Randomly sample 'batch size' experiences\n",
    "        idx = random.choices(range(self.num_in_queue), k=self.memory_sample_size)\n",
    "        \n",
    "        STATE = self.STATE_MEM[idx].to(self.device)\n",
    "        ACTION = self.ACTION_MEM[idx].to(self.device)\n",
    "        REWARD = self.REWARD_MEM[idx].to(self.device)\n",
    "        STATE2 = self.STATE2_MEM[idx].to(self.device)\n",
    "        DONE = self.DONE_MEM[idx].to(self.device)\n",
    "        \n",
    "        return STATE, ACTION, REWARD, STATE2, DONE\n",
    "        \n",
    "    def experience_replay(self):\n",
    "        \n",
    "        if self.step % self.copy == 0:\n",
    "            self.copy_model()\n",
    "\n",
    "        if self.memory_sample_size > self.num_in_queue:\n",
    "            return\n",
    "\n",
    "        STATE, ACTION, REWARD, STATE2, DONE = self.recall()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        # Double Q-Learning target is Q*(S, A) <- r + γ max_a Q_target(S', a)\n",
    "        target = REWARD + torch.mul((self.gamma * self.target_net(STATE2).max(1).values.unsqueeze(1)), 1 - DONE)\n",
    "\n",
    "        current = self.local_net(STATE).gather(1, ACTION.long())\n",
    "        loss = self.l1(current, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run() -> DQNAgent:\n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v3', apply_api_compatibility=True, render_mode='rgb_array')\n",
    "    env = make_env(env)#, 'training_videos')\n",
    "    observation_space = env.observation_space.shape\n",
    "    action_space = env.action_space.n\n",
    "    agent = DQNAgent(state_space=observation_space,\n",
    "                     action_space=action_space,\n",
    "                     max_memory_size=30000,\n",
    "                     batch_size=32,\n",
    "                     gamma=0.90,\n",
    "                     lr=0.00025,\n",
    "                     exploration_max=0.90,\n",
    "                     exploration_min=0.02,\n",
    "                     exploration_decay=0.99)\n",
    "    \n",
    "    num_episodes = 10\n",
    "    #env.reset()\n",
    "    total_rewards = []\n",
    "    \n",
    "    for ep_num in tqdm(range(num_episodes)):\n",
    "        state, info = env.reset()\n",
    "\n",
    "        # State is a LazyFrame\n",
    "        state = torch.Tensor(state[0].__array__() if isinstance(state, tuple) else state.__array__())\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            state_next, reward, terminal, trunc, info = env.step(int(action[0]))\n",
    "            total_reward += reward\n",
    "            \n",
    "            state_next = torch.Tensor(state_next[0].__array__() if isinstance(state_next, tuple) else state_next.__array__())\n",
    "            reward = torch.tensor([reward])#.unsqueeze(0)\n",
    "            \n",
    "            terminal = torch.tensor([int(terminal)])#.unsqueeze(0)\n",
    "            agent.remember(state, action, reward, state_next, terminal)\n",
    "            agent.experience_replay()\n",
    "            \n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "        print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxence/miniconda3/envs/rl/lib/python3.12/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/maxence/miniconda3/envs/rl/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/tmp/ipykernel_3185/641635113.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
      " 10%|█         | 1/10 [00:02<00:20,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward after episode 1 is 234.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:09<00:42,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward after episode 2 is 1007.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:13<00:32,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward after episode 3 is 584.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:36<01:11, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward after episode 4 is 596.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:42<00:49,  9.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward after episode 5 is 692.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:43<00:27,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward after episode 6 is 248.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:45<00:15,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward after episode 7 is 618.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:46<00:07,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward after episode 8 is 249.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:01<00:07,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward after episode 9 is 704.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:03<00:00,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward after episode 10 is 596.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_agent = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env, num_episodes=1):\n",
    "    \"\"\"Runs the trained agent in the environment without training.\"\"\"\n",
    "    \n",
    "    total_rewards = []\n",
    "    \n",
    "    for ep_num in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        state = torch.Tensor(state[0].__array__() if isinstance(state, tuple) else state.__array__())\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state, evaluate=True)  # Use deterministic policy\n",
    "            state_next, reward, done, _, info = env.step(int(action.item()))\n",
    "            done = done or info['time'] < 250\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = torch.Tensor(state_next[0].__array__() if isinstance(state_next, tuple) else state_next.__array__())\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Evaluation Episode {ep_num + 1}: Total Reward = {total_reward}\")\n",
    "    \n",
    "    print(f\"Average Reward over {num_episodes} episodes: {np.mean(total_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3185/641635113.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, device=self.device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Episode 1: Total Reward = 243.0\n",
      "Average Reward over 1 episodes: 243.0\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v3', apply_api_compatibility=True, render_mode=\"rgb_array\")\n",
    "env = make_env(env, video_dir_path='mario_videos')\n",
    "evaluate_agent(trained_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxence/miniconda3/envs/rl/lib/python3.12/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n",
      "/home/maxence/miniconda3/envs/rl/lib/python3.12/site-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/maxence/ENSTA_3A/autonomous_agents/project/mario_videos_test folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/maxence/miniconda3/envs/rl/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/maxence/miniconda3/envs/rl/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your mario video is saved in mario_videos_test\n",
      "Moviepy - Building video /home/maxence/ENSTA_3A/autonomous_agents/project/mario_videos_test/mario-video-Wed Feb 26 14:57:09 2025-episode-0.mp4.\n",
      "Moviepy - Writing video /home/maxence/ENSTA_3A/autonomous_agents/project/mario_videos_test/mario-video-Wed Feb 26 14:57:09 2025-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/maxence/ENSTA_3A/autonomous_agents/project/mario_videos_test/mario-video-Wed Feb 26 14:57:09 2025-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "video_dir_path = 'mario_videos_test'\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v3', apply_api_compatibility=True, render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=video_dir_path,\n",
    "    episode_trigger=lambda episode_id: True,\n",
    "    name_prefix='mario-video-{}'.format(time.ctime())\n",
    ")\n",
    "\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# run 1 episode\n",
    "env.reset()\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _, info = env.step(action)\n",
    "    if done or info['time'] < 250:\n",
    "        break\n",
    "print(\"Your mario video is saved in {}\".format(video_dir_path))\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
